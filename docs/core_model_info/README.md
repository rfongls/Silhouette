# Core Model Info

Recommended: TinyLLaMA or Phi-2 (1.3B) in GGUF format.

Use with llama.cpp for local inference on CPU/edge devices.